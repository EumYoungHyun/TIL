### 커버리지(Coverage)

추천 시스템에서는 정확도 만큼 중요한 개념이 커버리지이다.  
커버리지는 추천 시스템이 제공할 수 있는 아이템과 컨텐츠 전체간의 비율이다.

예제인 무비렌즈 데이터를 생각해보면 데이터에는 영화들의 별점들이 기록되어 있지만 단 하나의 별점도 없는 데이터들도 분명 많이 존재한다. 이를 전체 영화들의 데이터에 대입한다면 전체 영화에 비해 추천되는 영화는 턱없이 부족한 상황이 발생한다. 이때 이 추천 시스템의 커버리지는 낮다고 판단할 수 있다.

커버리지의 개념은 정확성의 개념과 상충된다.
만약 추천시스템의 품질을 높이고 싶다면 정확성이 올라간 만큼 커버리지는 낮아질 것이다.

커버리지는 새로운 아이템이 얼마나 추천시스템에 추천되는지에도 중요한 지표이다.
음악이 신곡이 나왔다면 이를 누군가 들어야만 동향데이터가 생기고 추천이 가능해지기 때문이다.  
<br />

### 다양성 (Diversity)

추천 시스템이 얼마나 다양한 범주의 컨텐츠를 사용자에게 제공하는지에 대한 지표  
만약 어떤 서비스가 사용자가 읽은 시리즈만을 추천하고 비슷한 컨텐츠들은 추천하지 않는 다면 이는 다양성이 낮은 서비스이다.

이를 측정할때는 데이터를 번갈아 짝을 지어 유사성을 계산하고,  
그 값들의 평균값으로 유사성 S을 구할 수 있다.

다양성은 유사성과 반대되는 개념이기에 아래 식으로 표현할 수 있다.

$$
    1-{S}
$$

다양성은 추천 시스템에서 좋은 지표는 아니다.  
다양성이 높다는 것은 아무거나 추천한다는 뜻이기도 하기 때문이다.  
<br />

### 참신성 (Novelty)

참신성 또한 좋은 말 처럼 들릴 수 있으나 항상 좋은 개념이 아니다.
추천시스템에서의 참신성은 얼마나 대중적인 것을 추천하는지에 대한 지표이다.  
대부분의 데이터는 대중적이지 않기 때문이다.

여기에서는 사용자 신뢰라는 개념을 이용한다.  
사람들은 익숙한 컨텐츠를 추천받으면, 추천시스템이 잘 작동한다고 생각한다.  
사실 이는 대중적이라는 말 자체가 가진 의미이다.  
여기에 얼마나 대중적이지 않은 데이터들을 제공할 수 있는가가 추천시스템에서 중요한 지점이다.  
익숙한 데이터들로 사용자 신뢰를 얻고, 참신성있는 데이터를 섞어 신선함을 제공하여야 한다.

참신성이 중요한 이유는 추천시스템의 핵심이 '롱테일' 전략으로 데이터를 제공하는 것 이기 때문이다.

![Long tail](https://eumericano.s3.ap-northeast-2.amazonaws.com/dev/longtail.png "Long tail")

컨텐츠들이 채택되는 숫자를 내림차순으로 나열해보면
소수의 잘 팔리는 컨텐츠들(파란색)과 대다수의 일반 컨텐츠들(초록색)로 나열된다.  
인기 컨텐츠들은 사용자들에게 신뢰성을 주고,  
롱테일 컨텐츠들은 사용자들의 틈새 수요를 충족시켜 준다.

이를 잘 활용하면 컨텐츠 제공자들에게도 다양한 기회를 줄 수 있고,  
더 많은 컨텐츠 제공자를 만들며, 더 많은 컨텐츠, 더 많은 이용자를 창출해 낼 수 있다.

이 균형점을 찾는것이 추천 시스템이고 하나의 예술행위인 이유이다.  
<br />

### 이탈성 (churn)

사용자 추천 시스템은 얼마나 자주 변할까?  
이탈성을 토대로 추천 시스템이 얼마나 사용자의 새로운 행위에 민감한지
어느정도 측정 할 수 있다.

예를들어 사용자가 하나의 영화에 만점 별점을 주었을 때,  
사용자의 추천목록의 변화가 심하다면 이는 이탈성이 높다는 의미이다.

이탈성이 낮아 사용자에게 계속해서 같은 컨텐츠만 추천한다면 이는 쉽게 질려버리는 서비스가 된다.  
가끔 우선 추천 목록에 무작위 항목을 섞어 넣는것도 새로워 보일 수 있고,  
더 참신성 있는 데이터를 제공할 수 있기때문이다.

이탈성 또한 다양성과 참신성처럼 높은 지표가 꼭 좋지만은 않다.  
이탈성이 높다면 다른 지표들처럼 뜬금없는 데이터들로만 추천목록을 채울 수 있기 때문이다.
이런 종류의 메트릭들을 균형을 맞추어 묶어서 고려해야한다.  
<br />

### 민감성 (responsiveness)

민감성은 사용자의 새로운 행위가 얼마나 빠르게 추천 시스템에 도입되는가를 나타내는 지표이다.  
지금 본 영화가 바로 다음 추천부터 영향을 미칠까?  
아니면 하루간의 어떤 작업을 통해 추천 시스템에 영향을 미치게 될까?

민감성 값이 높은게 좋다고 느껴질 수 있으나 사업을 진행하면서 어느정도 민감성을 조절해야 한다.  
민감성이 높다면 시스템 유지가 어려워지고, 시스템 구성의 비용이 증가하게 된다.  
따라서 사업주는 민감성과 간소성에서 적절한 균형값을 찾아야 한다.

---

## 다양한 지표들을 내 서비스에 맞게 조율하는 방법

### A/B 테스트

지금까지 익힌 추천 시스템들 중 어느 지표에 좀 더 신경을 써야할까?  
당연하겠지만 상황에 따라 다르다.  
고객군의 특성에 따라도 다르며 사업의 목표에 따라도 달라질 수 있다.  
어떤 방식이 적합한지 테스트 하는 방식으로 A/B 테스트가 있다.  
실제 고객에게 맞추어 추천을 수정하고, 수정된 추천 시스템에 고객들이 어떻게 반응하는지 다시 측정해야 한다.

사용자들이 더 구매하는지, 얼마나 훑어보는지, 아니면 다른 방식으로 관심을 표현하는지 통제 가능한 온라인 실험을 통해 행동 패턴을 관찰할 수 있다.

당연한 이야기지만 모든 접근을 사업적으로 생각해야 한다.  
고도화된 알고리즘과 어렵고 복잡한 적용방식들을 대입한다고 하더라도,  
사용자들이 실제로 더 쉽게 이탈하고 구매하지 않는 알고리즘은 바로 제거해야 한다. 즉, 사용자 행동만이 시험의 궁극적 대상임을 인지해야 한다.  
알고리즘의 복잡성은 그대로 비용과 직결되기 때문에 온라인 A/B 테스트를 통해 값의 변화가 없는 복잡한 방식들을 제거해야 한다.

이전에 살펴보았던 넷플릭스의 사례처럼 정확성 메트릭에서의 높은 값들도 종종 온라인에서 실패하는 경우가 다반사다.  
유튜브 또한 이를 연구했는데, 이런 현상을 '대리 문제'라고 한다.  
평가를 공식에 정확히 측정했음에도 영상 추천 결과가 좋지 못할 수 있다는 것이다.  
유튜브에서는 'There is more art than science in selecting the surrogate problem for recommandations' 라고 언급 했다.
넷플릭스 또한 앞에서 언급했듯 비슷한 결론을 냈다.  
결국 가장 중요한 것은 온라인 A/B 테스트의 결과가 추천 시스템에서 가장 중요한 평가 지표라는 것이다.  
<br />

### 품질 측정 (perceived quality)

또 다른 방법으로는 사용자들에게 직접 특정 추천 목록이 좋았는지 물어보는 것이다.  
이는 항목의 평점을 받아서 외재적 피드백을 얻는 것과 같다. 앱 리뷰를 써달라든지, 설문조사를 통해 시스템을 평가해 달라는 식이다.

품질의 측정 혹은 추천 측정이라고 한다.  
좋은 추천이라는 것은 결코 확실한 것이 아니까 이를 직접 묻는 방식은 꽤 괜찮은 방식이 될 수 있다.  
하지만 여기서도 유념해야할 개념이 있다.  
사용자들은 평가해야 할 대상이 그 항목 자체의 품질인지 아니면 그것이 좋은 추천 항목이었는지를 혼동할 수 있다는 점이다.  
그래서 평가 데이터를 해석하는 것이 힘들거나 틀리기 쉽다.  
그리고 명확한 평가를 주지 않은 고객의 데이터를 해석하는 데 오랜 시간이 걸릴 것이다.  
그래서 고객들이 추천 항목에 얼마나 돈을 써서 품질을 평가했는지 측정해보는 것처럼 암시적 평가를 통한 온라인 A/B 테스트가 최선이라는게 현재 업계의 주요한 의견이다.
