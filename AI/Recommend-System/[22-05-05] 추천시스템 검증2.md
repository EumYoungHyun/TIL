### 추천시스템 정확도 판단 (MSE, RMSE, MAE)

검증력을 수학적으로 판단하는 방법이 있다.  
머신러닝의 모델을 최적화 하는데 사용되는 평가지표로서  
많이 사용되는건 MSE MAE RMSE들이고, 이중에서도 RMSE를 많이 사용한다

![MSE MAE RMSE](https://eumericano.s3.ap-northeast-2.amazonaws.com/dev/MSE%2C+MAE%2C+RMSE.png "MSE MAE RMSE")

코드 예시)

```python
import matplotlib
import numpy as np

def MSE(pred,target,epochs=len(pred)):
    losses=[]
    for i in range(epochs):
        losses.append(np.sum((pred[i]-target[i])**2)/len(pred))
    return losses

def RMSE(pred,target,epochs=len(pred)):
    losses=[]
    for i in range(epochs):
        losses.append(np.sqrt(np.sum((pred[i]-target[i])**2)/len(pred)))
    return losses

def MAE(pred,target,epochs=len(pred)):
    losses=[]
    for i in range(epochs):
        losses.append(np.sum(np.abs(pred[i]-target[i]))/len(pred))
    return losses

pred = np.array([[0,4,9],[2,4,2],[3,5,6]])
target = np.array([[3,5,7],[3,5,7],[3,5,7]])

print(MSE(pred,target))
print(RMSE(pred,target))
print(MAE(pred,target))

# outputs:
# [4.666666666666667, 9.0, 0.3333333333333333]
# [2.160246899469287, 3.0, 0.5773502691896257]
# [2.0, 2.3333333333333335, 0.3333333333333333]
```

1. MSE (Mean Squared Error, 평균 제곱 오차)  
   평균 제곱오차는 예측값과 정답과의 차이를 제곱하기 때문에 이상치에 민감하다.
   즉, 다른 데이터들은 비교적 비슷하더라도 하나의 큰 이상치가 있다면 그 값에 대해 민감하게 평가한다.  
   제곱이므로 소숫점의 차이는 더 작게 반영하는 특징이 있으며, 이차함수로서 첨점이 없고, 모든 점에서 미분 가능하다.

2. RMSE (Root Mean Squared Error, 평균 제곱근 오차)  
   평균 제곱근 오차는 MSE의 제곱근을 구하는 방식으로 MSE에 비해 비교적 큰차이에 덜 민감하다.  
   MSE에 루트를 취하기 때문에 일차함수 특징을 가지고 있고, 첨점이 있어 특정 지점에서는 미분 불가능하다.

3. MAE(Mean Absolute Error, 평균 절대값 오차)  
   평균 절대값 오차는 위 두가지 오차값과는 다르게 블랙스완에 강한 지표이다.  
   이 또한 절대값이므로 RMSE처럼 첨점이 있어 특정 지점에서는 미분이 불가능하고,  
   좀 더 디테일한 차이에 집중하는 느슨한 평가 지표가 될 수 있다.

이러한 측정들은 오프라인(개발환경)에서 평가하기 어렵다. 여기에는 다양한 견해들이 존재하는데  
일례로 2006년에 넷플릭스에서 추천알고리즘 개선에 현상금 100만 달러를 걸었다.  
이떄의 평가기준은 낮은 RMSE였는데, 대회를 주체한 넷플릭스 조차도 가장 낮은 RMSE를 달성한 연구결과를 추천 서비스에 반영하지 않았다.  
그 이유는 현실세계에서 고객의 사용 패턴을 보니 RMSE 지표는 생각보다 중요한 지표가 아니었음을 느꼈기 떄문이다.

중요한점은 단순히 낮은 지표를 기록하는게 아니라 사용자들이 클릭할만한 컨텐츠를 두는것이 더 바람직함을 깨닳았기 떄문이다.

---

<br />

### TOP N 적중률

2009년 이후로 넷플릭스는 추천에 더욱 집중된 측정법에 기반해 넷플릭스 상을 고려하게 되었다.  
이런 방식은 몇가지 없는데, 첫번째 방법은 hit rate이다.

<br />

#### 1. Hit Rate

시험 집단 안에 있는 모든 사용자에 대한 순위 추천을 발생시킨다.  
그 중 사용자가 실제로 평가한 지표가 있다면 이를 hit이라고 기록한다.
모든 시험 집단 중 힛 기록을 더하여 유저수로 나눈 값이 hit rate이다.

$$
    Hit Rate =  \left( \frac{hits}{users} \right)
$$

hit rate는 이해하기는 쉬우나 이것을 측정하는 것은 쉽지 않다.  
이는 정확도나 개별평점을 측정하는 것이 아니기 때문에 train/test set이나 cross validation을 사용해 평가할 수 없다.

우리가 평가하고자 하는 것은 개별 사용자들의 순위 목록에 대한 정확도를 평가하는 것 이기 때문에,  
굳이 따로 평가하지 않고 train한 데이터를 통해 추천의 적중률을 테스트 할 수 있겠으나,  
이는 엄밀히 말하면 속임수에 불과하다.  
일반적으로 추천 시스템이 훈련한 데이터로 평가하는 것은 올바르지 않다.  
마치 정답지만을 공부하고 이로 시험을 보는 기사 자격증 필기 시험처럼, 이론에 대해 공부할 수 있겠으나  
기술에 대한 정확한 평가 방식이라고 하기엔 문제가 있다.

이 문제를 해소할 다른 영리한 방식은 leave-one-out cross validation이다.

#### 2. Leave-One-Out Cross Valistation

이 방식은 위에서 언급한 단순한 방식으로

1. 의도적으로 사용자의 훈련 데이터에서 한 항목을 제거하고,
2. Hit Rate를 계산할 때 처럼 모든 순위 추천을 발생시킨 후,
3. 추천 알고리즘에서 임의로 제거한 항목을 추천해주는지를 평가하는 방식이다.

제작하는 추천 시스템이 현실 세계의 데이터를 반영하는 것을 염두해두고 개발이 된다면,  
더 사용자의 경험에 기반한 추천 시스템이 될 수 있다.

이 방식의 문제는, 그냥 하나의 추천 영화를 가져오는 것 보다  
검증을 하면서 하나의 특정한 영화를 알맞게 가져오는게 더 어려워졌다는 점이다.  
그러므로 이 방식은 아주 큰 data-set이 존재하지 않는다면 측정하기 어려운 경향이 있다.
적중률에 대한 분산을 평군 상호적중률 (ARHR) 이라고 부른다.

<br />

#### 3. ARHR (Average Reciprocal Hit Rate, 평균 상호 적중률)

![ARHR](https://eumericano.s3.ap-northeast-2.amazonaws.com/dev/arhr.png "ARHR")

이 측정법은 적중률 계산과 똑같은데, 적중에 나타나는 최상위 목록을 설명한다.  
즉, 하위 슬롯들보다는 상위 슬롯에서의 더 높은 신뢰도를 얻을 수 있다.

이것은 Leave-One-Out Cross Validation보다 한번 더 사용자 중심적인 방식이다.  
왜냐하면 실 사용자들은 추천 목록에서 시작부분에 더 집중하는 경향을 보이기 때문이다.

유일한 차이점은 hit 갯수를 더하는 것이 아닌 각 적중의 상호순위를 더하는 것이다.

그래서 만약 우리가 3번 슬롯에서 성공적으로 추천을 예측했다면 1/3로 간주되지만,  
최상위 추천은 1로 간주된다.

이 방식은 사용자가 더 많은 추천을 보기 위해 스크롤하거나 페이지를 옮겨다녀야 한다면,  
아예 아무런 인터렉션 없이 추천을 받을 수 있도록 하는 사용자 경험에 더 집중한 형태이다.

<br />

#### 4. cHR (cumulative Hit Rate, 누적 추천률)

topN을 먼저 계산하고 사용자의 예상 평점을 함께 계산해 높은 추천 순위임에도 사용자의 만족도가 낮을것으로 예측된다면 이 항목을 버리는 방식입니다.

![cHR](https://eumericano.s3.ap-northeast-2.amazonaws.com/dev/chr.png "cHR")

사진은 일부 hit rank와 예측별점의 예시입니다. 우리의 시스템이 만약 별점 3점이라는 커트라인을 가지고 있다면 hit rank는 높지만 예측별점이 낮은 2, 4번째의 추천 항목을 버려 사용자에게서 서비스 신뢰도를 깎지 않는것을 목적으로 합니다.

### 5. rHR (rating Hit Rate, 평점 적중률)

적중률을 살펴보는 다른 방법은 hit rate를 예상 별점으로 쪼개어 보는 방식이다.

![rHR](https://eumericano.s3.ap-northeast-2.amazonaws.com/dev/rHR.png "rHR")

별점별로 hir rate를 쪼개어 사용자가 실제 선호하는 컨텐츠를 잘 추천받고 있는지를 더 디테일하게 분석할 수 있는 지표이다.

위의 지표들, 아이디어들은 모두 오프라인(개발환경)에서 추천의 유효성을 평가하는 다른 방식들이다.  
RMSE의 작은 개선이 중요한 지표인 적중률에 많은 개선을 불러 일으킨다는 점이 밝혀졌다.  
또한 적중률과 RMSE 점수가 항상 관계있지 않다는 점도 알 수 있다.
